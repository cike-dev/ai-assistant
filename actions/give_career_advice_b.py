import os
import sys
import asyncio
from typing import Any, Text, Dict, List

from ddgs import DDGS                     # sync library
import google.generativeai as genai       # sync library

from rasa_sdk import Action, Tracker
from rasa_sdk.executor import CollectingDispatcher
from rasa_sdk.events import SlotSet

from .logger_utils import get_logger

# Initialize a logger for this global setup block
logger = get_logger("Career Advice action")


class ActionGiveCareerAdvice(Action):
    """
    Async Rasa action that:
      1. Builds a UK‚Äëfocused DuckDuckGo query
      2. Retrieves the top 5 snippets (run in a thread)
      3. Sends those snippets to Gemini‚ÄëPro (also run in a thread)
      4. Returns bullet‚Äëpoint advice + sources
    """

    def name(self) -> Text:
        return "action_give_career_advice"

    # ---------------------------------------------------------
    # Helper ‚Äì run the **blocking** DDGS search in a thread
    # ---------------------------------------------------------
    @staticmethod
    def _search_ddgs(query: str) -> tuple[list[str], list[str]]:
        """
        Returns (snippets, sources).  Raises if no results.
        """
        results = list(
            DDGS().text(
                query,
                max_results=8,
                region="uk-en",          # bias toward the United Kingdom
                safesearch="moderate",
            )
        )

        snippets, sources = [], []
        for result in results[:5]:          # keep only the 5 most relevant
            body = result.get("body")
            href = result.get("href")
            
            if body:
                snippets.append(body)
            
            if href:
                sources.append(href)

        if not snippets:
            raise ValueError("No relevant search results found.")
            ## TODO: Use logger or slot events to handle this error!
            logger.error("_search_ddgs: No relevant search results found!")
        return snippets, sources

    # ---------------------------------------------------------
    # Helper ‚Äì call Gemini in a thread
    # ---------------------------------------------------------
    @staticmethod
    def _call_gemini(prompt: str, api_key: str) -> str:
        genai.configure(api_key=api_key)

        # You can swap model names ‚Äì `gemini-pro` gives higher quality than flash
        model = genai.GenerativeModel("gemini-pro")

        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.7,
                top_p=0.8,
                max_output_tokens=500,
            ),
        )
        return response.text.strip()

    # ---------------------------------------------------------
    # Async entry point
    # ---------------------------------------------------------
    async def run(
        self,
        dispatcher: CollectingDispatcher,
        tracker: Tracker,
        domain: Dict[Text, Any],
    ) -> List[Dict[Text, Any]]:
        
        # Check for env var early, fail fast
        GEM_API_KEY = os.getenv("GEMS")
        if not GEM_API_KEY:
            logger.error("Gemini API key [GEM_API_KEY] missing.")
            # return [SlotSet("config_error", True)]

        # -----------------------------------------------------------------
        # Pull slots (no async work needed here)
        # -----------------------------------------------------------------
        major = tracker.get_slot("current_major") or "your field of study"
        interest = tracker.get_slot("career_interest") or "career path"
        year = tracker.get_slot("year_of_study") or "current year"
        gpa = tracker.get_slot("gpa") or "N/A"

        # Normalise the internship flag
        has_internship_slot = tracker.get_slot("has_internship")
        has_internship = (
            str(has_internship_slot).lower() in ["true", "yes", "1", "y"]
            if has_internship_slot is not None
            else False
        )

        # -----------------------------------------------------------------
        # Build a UK‚Äëspecific query
        # -----------------------------------------------------------------
        query = (
            f"2025 {interest} career advice {major} {year} student "
            f"{'with internship experience' if has_internship else 'entry-level'} "
            f"site:.ac.uk OR site:.gov.uk OR site:linkedin.com/company/uk- OR site:indeed.co.uk"
        )

        # -----------------------------------------------------------------
        # Async operation: Run the *blocking* search in a thread (non‚Äëblocking for Rasa)
        # -----------------------------------------------------------------
        try:
            snippets, sources = await asyncio.to_thread(self._search_ddgs, query)
        except Exception as e:
            # ---------- Fallback when the web search fails ----------
            advice_text = f"""
Sorry, I couldn‚Äôt fetch fresh web data right now. Here‚Äôs solid, UK‚Äëfocused guidance for {interest}:

‚Ä¢ **2025 Trends** ‚Äì Look for roles in cybersecurity, AI, data science, and cloud engineering üöÄ
‚Ä¢ **Key Skills** ‚Äì Strengthen Python, cloud (AWS/Azure), and data‚Äëvisualisation (Power‚ÄØBI/Tableau) üìö
‚Ä¢ **Experience** ‚Äì Try personal projects, open‚Äësource contributions, or volunteer work üíº
‚Ä¢ **Networking** ‚Äì Join LinkedIn UK groups, attend the University of Wolverhampton Careers Fair, and connect with alumni ü§ù

The University‚Äôs Careers Service can give you personalised advice! üéì
"""
            sources_list = "General UK resources: National Careers Service, Prospects, LinkedIn Learning"
            # Send the fallback and exit early
            dispatcher.utter_message(text=advice_text)
            dispatcher.utter_message(text=f"Sources:\n{sources_list}")
            dispatcher.utter_message(text="Anything else I can help you with?")
            return []

        # -----------------------------------------------------------------
        # Build the *prompt* that will be fed to Gemini
        # -----------------------------------------------------------------
        
        prompt = f"""
You are a warm, encouraging career counsellor at the University of Wolverhampton.
A {year} {major} student (GPA {gpa}) is asking for advice about {interest}.
{'They have internship experience.' if has_internship else 'They are looking for entry‚Äëlevel opportunities.'}

Using ONLY the following UK‚Äëfocused search results, write 4‚Äë5 upbeat bullet points:
- Simple, friendly language with 1‚Äë2 emojis per point
- Highlight 2025 job prospects, must‚Äëhave skills, internship ideas, and CV/LinkedIn tips for the UK market
- End with a short networking suggestion (e.g., LinkedIn groups, university alumni events)

Search results:
{chr(10).join(snippets)}

Advice:
"""
        # -----------------------------------------------------------------
        # Call Gemini in a thread (again non‚Äëblocking)
        # -----------------------------------------------------------------
        
        # --- In order to fail fast, this check was done earlier --- #
        # GEM_API_KEY = os.getenv("GEMS")
        # if not GEM_API_KEY:
        #     raise ValueError("Environment variable GEMS (Gemini API key) is missing.")

        try:
            advice_text = await asyncio.to_thread(
                self._call_gemini, prompt, GEM_API_KEY
            )
        except Exception as e:
            # ---------- Gemini failure fallback ----------
            advice_text = f"""
I ran into a problem while generating the answer. Here‚Äôs a quick, timeless UK‚Äëfocused tip for {interest}:

‚Ä¢ Focus on building Python, cloud, and data‚Äëanalysis skills.
‚Ä¢ Look for graduate schemes on Prospects, TargetJobs, and the university‚Äôs job board.
‚Ä¢ Start a small personal project and showcase it on GitHub/LinkedIn.

The Careers Service at Wolverhampton can help you tailor this further! üéì
"""
            sources_list = "General UK resources: National Careers Service, Prospects, LinkedIn Learning"
            dispatcher.utter_message(text=advice_text)
            dispatcher.utter_message(text=f"Sources:\n{sources_list}")
            dispatcher.utter_message(text="Let me know if you‚Äôd like more details on any point.")
            return []

        # -----------------------------------------------------------------
        # Prepare a tidy sources list for the user
        # -----------------------------------------------------------------
        sources_list = "\n".join([f"‚Ä¢ {src}" for src in sources if src])

        # -----------------------------------------------------------------
        # Send everything back to the user ‚Äì keep the conversation natural
        # -----------------------------------------------------------------
        # a ‚Äì the advice (the LLM output)
        dispatcher.utter_message(text=advice_text)

        # b ‚Äì a short ‚Äúhere are the sources‚Äù line (optional, but nice for transparency)
        dispatcher.utter_message(text=f"Sources for this advice:\n{sources_list}")

        # c ‚Äì a gentle follow‚Äëup that encourages the user to keep chatting
        dispatcher.utter_message(
            text="Hope that helps! Feel free to ask for more detail on any point or about other career options."
        )

        ### OR: ###
        # full_message = f"{advice_text}\n\n**Sources:**\n{sources_list}\n\nHope that helps! Feel free to ask for more detail."
        # dispatcher.utter_message(text=full_message)

        return []   # no events to set, just plain messages