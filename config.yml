# # The config recipe.
# recipe: default.v1

# # The assistant project unique identifier
# # This default value must be replaced with a unique assistant name within your deployment
# assistant_id: placeholder_default

# language: en
# pipeline:
# - name: CompactLLMCommandGenerator
#   llm:
#     model_group: openai-gpt-4o

# # Configuration for Rasa Core.
# policies:
# - name: FlowPolicy
# - name: IntentlessPolicy



# ================================================================ #
# My config recipe
# ================================================================ #
# The config recipe.
recipe: default.v1

# The assistant project unique identifier
# This default value must be replaced with a unique assistant name within your deployment
assistant_id: wlv_chat_assistant

language: en

# SearchReadyLLMCommandGenerator is better suited for EnterpriseSearch
# it optimizes RAG-based knowledge retrieval and business flow accuracy 
# better than CompactLLMCommandGenerator

# NLG configs, powered by LLMs
pipeline:
  - name: SearchReadyLLMCommandGenerator
    user_input:
      max_characters: 512 # default value: 420
    llm:
      model_group: gemini_pipeline # use LLM model_group ID defined in endpoints.yml
    flow_retrieval:
      turns_to_embed: 1
      should_embed_slots: true
      num_flows: 5 # or 20
      embeddings:
        model_group: gemini_pipeline_embed  # Use embedding model id in endpoints.yml


# Configuration for Rasa Core.
policies:
  - name: FlowPolicy
    llm:
      model_group: gemini_policy  # LLM model id in endpoints.yml

  - name: EnterpriseSearchPolicy
    check_relevancy: true
    # citation_enabled: true
    vector_store: 
      type: "faiss"
      source: "./docs"
      threshold: 0.6
    llm:
      model_group: mistral_llm  # LLM model id in endpoints.yml
    embeddings:
      model_group: mistral_rag  # embedding model id in endpoints.yml
    


