# This file contains the different endpoints your bot can use.

# Server where the models are pulled from.
# https://rasa.com/docs/rasa-pro/production/model-storage#fetching-models-from-a-server

#models:
#  url: http://my-server.com/models/default_core@latest
#  wait_time_between_pulls:  10   # [optional](default: 100)

# Server which runs your custom actions.
# https://rasa.com/docs/rasa-pro/concepts/custom-actions

action_endpoint:
  actions_module: "actions"

# Tracker store which is used to store the conversations.
# By default the conversations are stored in memory.
# https://rasa.com/docs/rasa-pro/production/tracker-stores

#tracker_store:
#    type: redis
#    url: <host of the redis instance, e.g. localhost>
#    port: <port of your redis instance, usually 6379>
#    db: <number of your database within redis, e.g. 0>
#    password: <password used for authentication>
#    use_ssl: <whether or not the communication is encrypted, default false>

#tracker_store:
#    type: mongod
#    url: <url to your mongo instance, e.g. mongodb://localhost:27017>
#    db: <name of the db within your mongo instance, e.g. rasa>
#    username: <username used for authentication>
#    password: <password used for authentication>

# Event broker which all conversation events should be streamed to.
# https://rasa.com/docs/rasa-pro/production/event-brokers

#event_broker:
#  url: localhost
#  username: username
#  password: password
#  queue: queue



# The lines below activate contextual rephrasing, using the default OpenAI language model.
# Ensure the OPENAI_API_KEY is set to prevent any missing API key errors.
# For more details, refer to the documentation:
# https://rasa.com/docs/rasa-pro/concepts/contextual-response-rephraser
# To enable the rephraser, remove the comment symbols in the lines below.
nlg:
  type: rephrase
  # rephrase_all: true # uncomment to rephrase all bot responses
  llm:
    model_group: gemini_llm



# model_groups:
#   - id: openai-gpt-4o
#     models:
#       - provider: openai
#         model: gpt-4o-2024-11-20
#         request_timeout: 7
#         max_tokens: 256

model_groups:
  - id: huggingface_llm
    models:
      - provider: huggingface
        # model: meta-llama/CodeLlama-7b-Instruct-hf 
        model: Geetansh007/Counsellor
        api_base: "https://my-endpoint.huggingface.cloud"  #api where the is model hosted
        api_key: ${HF_TOKEN}
        # note that the huggingface model must be gguf type

  - id: ollama_local
    models:
      - provider: ollama
        model:  ollama_chat/llama3.2
        api_base: "http://localhost:11434"  # default ollama api
        # to use this model group, ensure this model is running in ollama
        request_timeout: 20

  - id: gemini_llm
    models:
      - provider: gemini
        model: gemini-2.5-flash
        api_key: ${GEMINI_API_KEY}  # Optional, set as env variable or here
        # request_timeout: 20

  - id: gemini_embedding
    models:
      - provider: gemini
        model: gemini-embedding-001
        api_key: ${GEM_EMBED_API_KEY}  # Optional, set as env variable or here
        # request_timeout: 20

  - id: mistral_llm
    models:
      - provider: mistral
        model: mistral-small-latest
        api_key: ${MISTRAL_API_KEY} # Optional, if you want to set the API key in the model configuration.
        request_timeout: 20
  
  - id: mistral_embedding
    models:
      - provider: mistral
        model: mistral-embed
        api_key: ${MISTRAL_API_KEY} # Optional, if you want to set the API key in the model configuration.
        request_timeout: 20

# vector_store:
#   # For Faiss (in-memory, local)
#   type: "faiss"
  
  # Add retrieval files to /docs, in .txt format
  # For Milvus or Qdrant, add host, port, collection, etc.
  