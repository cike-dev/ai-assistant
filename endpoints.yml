# This file contains the different endpoints your bot can use.

# Server where the models are pulled from.
# https://rasa.com/docs/rasa-pro/production/model-storage#fetching-models-from-a-server

#models:
  # url: http://my-server.com/models/default_core@latest
  # wait_time_between_pulls:  10   # [optional](default: 100)


# Tracker store which is used to store the conversations.
# By default the conversations are stored in memory.
# https://rasa.com/docs/rasa-pro/production/tracker-stores

#tracker_store:
  #  type: redis
  #  url: <host of the redis instance, e.g. localhost>
  #  port: <port of your redis instance, usually 6379>
  #  db: <number of your database within redis, e.g. 0>
  #  password: <password used for authentication>
  #  use_ssl: <whether or not the communication is encrypted, default false>

#tracker_store:
  #  type: mongod
  #  url: <url to your mongo instance, e.g. mongodb://localhost:27017>
  #  db: <name of the db within your mongo instance, e.g. rasa>
  #  username: <username used for authentication>
  #  password: <password used for authentication>

# Event broker which all conversation events should be streamed to.
# https://rasa.com/docs/rasa-pro/production/event-brokers

# event_broker:
  # url: localhost
  # username: username
  # password: password
  # queue: queue

# # Tracing for collecting performance metrics data.
# tracing:
#   type: jaeger
#   host: localhost
#   port: 14250
#   service_name: rasa
#   sync_export: ~

# Server which runs your custom actions.
# https://rasa.com/docs/rasa-pro/concepts/custom-actions

action_endpoint:
  actions_module: "actions"

# The lines below activate contextual rephrasing, using the default OpenAI language model.
# Ensure the OPENAI_API_KEY is set to prevent any missing API key errors.
# For more details, refer to the documentation:
# https://rasa.com/docs/rasa-pro/concepts/contextual-response-rephraser

# To enable the rephraser, remove the comment symbols in the lines below.
nlg:
  type: rephrase
  llm:
    model_group: gemini_rephraser
  prompt_template: prompts/rephraser.jinja2
  rephrase_all: False 
  summarize_history: False 

model_groups:
  - id: gemini_rephraser
    models:
      - provider: gemini
        model: gemini-2.5-flash
        api_key: ${GEMINI_REPHRASER}

  - id: gemini_pipeline
    models:
      - provider: gemini
        model: gemini-2.5-flash
        api_key: ${GEMINI_PIPELINE}  # Optional, set as env variable or here
        # request_timeout: 20

  - id: gemini_pipeline_embed
    models:
      - provider: gemini
        # model: gemini-embedding-001
        model: gemini-embedding-001
        api_key: ${GEMINI_PIPELINE}  # Optional, set as env variable or here
        # request_timeout: 20

  - id: gemini_policy
    models:
      - provider: gemini
        model: gemini-2.5-flash
        api_key: ${GEMINI_POLICY}

  - id: gemini_policy_embed
    models:
      - provider: gemini
        # model: gemini-embedding-001
        model: gemini-embedding-001
        api_key: ${GEMINI_POLICY}

  - id: mistral_llm
    models:
      - provider: mistral
        model: mistral-small-latest
        api_key: ${MISTRAL_API_KEY} # Optional, if you want to set the API key in the model configuration.
        # request_timeout: 20
  
  - id: mistral_rag
    models:
      - provider: mistral
        model: mistral-embed
        api_key: ${MISTRAL_API_KEY} # Optional, if you want to set the API key in the model configuration.
        # request_timeout: 20


# model_groups:
  # - id: openai-gpt-4o
  #   models:
  #     - provider: openai
  #       model: gpt-4o-2024-11-20
  #       request_timeout: 7
  #       max_tokens: 256

  # - id: huggingface_llm
  #   models:
  #     - provider: huggingface
  #       # model: meta-llama/CodeLlama-7b-Instruct-hf 
  #       model: Geetansh007/Counsellor
  #       api_base: "https://my-endpoint.huggingface.cloud"  #api where the is model hosted
  #       api_key: ${HF_TOKEN}
  #       # note that the huggingface model must be gguf type

  # - id: ollama_local
  #   models:
  #     - provider: ollama
  #       model:  ollama_chat/llama3.2
  #       api_base: "http://localhost:11434"  # default ollama api
  #       # to use this model group, ensure this model is running in ollama
  #       request_timeout: 20
  
